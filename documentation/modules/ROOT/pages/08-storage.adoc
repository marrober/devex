= Adding Container Storage
:navtitle: Adding container Storage

[#ephemeral_container_storage]
== Ephemeral Container Storage

Containers by their nature are ephemeral – they are short lived compute units with no data persistence. Any data a container works with or produces is held within runtime memory – as a result of which stopping or restarting a container results in data loss.

Although containers appear to have a file system, this by default is an in memory file system. In the following screen shot, you can see the root directory structure from within our container, which is used by our java application - this is all within volatile memory.

image::08-01-container-filesystem.png[container root filesystem,700,align="center"]


[#container_volumes]
== Container Volumes


However, applications are only part of the puzzle when trying to build something useful – we need data. Part of this challenge is resolved by using volumes, which can be attached to containers. In its simplest form, a container volume is simply a folder accessible to the host that the container runs on, which is mapped to a folder within the container. For example:


On host:

[.console-output]
[source,bash]
----
/mnt/data/
----

Within container:
[.console-output]
[source,bash]
----
/mnt/database/
----


Joining them together:

[.console-output]
[source,bash]
----
podman run –-name mycotainer -v /mnt/data:/mnt/database:Z mycontainer-image:v1
----

When the command above is run, a container called "mycontainer" is started on the host. It uses a container image called "mycontainer-image" with the tag "v1". It also maps the local folder /mnt/data to the folder within the container /mnt/database. The -v switch simply means "volume" and the ":Z" attribute is a security feature which we won't go into at this stage (it involves a security system called SELinux)

Once the container starts, if we were to look inside the container under /mnt/database, we would see that the content of that folder matches /mnt/data on the host. And crucially, we can both read and write data to this folder in real time. 

Now that the /mnt/database has persistence to a real storage device, each time the container stops and starts the pre-existing volume will be mounted and available for consumption by the container. This allows data persistence for any application that we want to design and run within a container ecosystem.


[#openshift_storage]
== OpenShift Storage

Local storage volumes are great for a small number of containers, but real world deployments of containerised applications can have hundreds if not thousands of containers running that all need persistent storage for data. This would quickly become difficult to manage manually using the methodology we have discussed so far. Happily, kubernetes and therefore Red Hat OpenShift has a technology which makes the provision and management of containerised storage relatively simple. It is called "Persistent Volumes" and "Persistent Volume Claims". A key thing to remember is however, that this technology still makes use of the volume mounts typology that we have briefly discussed "under the hood".

[#persistent_volumes]
== Persistent volumes

A PersistentVolume (PV) is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using Storage Classes. A StorageClass provides a way for administrators to describe the "classes" of storage they offer. Beneath the storage class, real storage drives are obsficated from the administrator using a container storage interface.-  This takes care of the complicated process of accounting for all of the different types of storage regardless of vendor or cloud platform, making it simple and easy to consume by developers on a Kuberentes platform.

A persistent volume is a resource in the cluster just like a node is a cluster resource. PVs are volume plugins like Volumes, but have a life cycle independent of any individual Pod that uses the PV. This API object captures the details of the implementation of the storage, be that NFS, iSCSI, or a cloud-provider-specific storage system.

[#persistent_volume_claims]
== Persistent Volume Claims

A PersistentVolumeClaim (PVC) is a request for storage by a user. It is similar to a Pod. Pods consume node resources and PVCs consume PV resources. Pods can request specific levels of resources (CPU and Memory). Claims can request specific size and access modes (e.g., they can be mounted ReadWriteOnce, ReadOnlyMany or ReadWriteMany, see AccessModes).

[#build_our_app]
== Build our App

To demonstrate how to use container storage with OpenShift, we will start by deploying a very simple web application container onto our cluster, and then interact with the local filesystem on that container via the terminal. To begin with, we will create a new project in OpenShift for this experiment. From the developer view within OpenShift, click the drop down towards the top of the window and then select "Create Project"

image::08-02-create-project.png[container root filesystem,300,align="center"]

We will call this project:

[.console-input]
[source,bash,subs="+attributes"]
----
bootcamp-webserver
----


image::08-03-create-project-name.png[Enter a project name,400,align="center"]


With a blank project created, we will now create a deployment from the developer view, using the import from git strategy. We will also specify a container file which will define and create our application for us. Start by selecting the "Import from Git" option (make sure you are in "Developer View") as shown below:

image::08-04-import-from-git.png[import from git,300,align="center"]

We will now enter the git repository url that contains the code we are going to deploy to our cluster:


[.console-input]
[source,bash,subs="+attributes"]
----
https://github.com/mkimberley/bootcamp_webserver.git
----

image::08-05-git-repo-name.png[container root filesystem,700,align="center"]

We need to click "Edit import Strategy" and choose "Dockerfile"


We now need to tell OpenShift that we wish to specify a container file. The file "Containerfile" is at the root of the git repository we are using, and contains the build steps to make our container image, the contents of which are as follows:

[.console-input]
[source]
----
FROM golang:latest

# Add Maintainer Info
LABEL maintainer="Matt Kimberley <mattkimberley84@gmail.com>"

# Copy the source
RUN mkdir /app
COPY src/ /app
COPY go.mod /app
WORKDIR /app

# Build the Go app
RUN go get github.com/mkimberley/bootcamp_websever
RUN go get github.com/gin-gonic/gin
RUN go build -o main .

# Create a working folder
RUN mkdir /data
RUN chmod -R 777 /data

# Expose port 8080 to the outside world
EXPOSE 8080/tcp

# Command to run the executable
CMD ["/app/main"]
----

In essence, the above choses a base container image to start from (golang:latest in our case) and then specifies a number of "layers" to add onto that base image which represent our customisation to deploy our application source code.

The last command defines what the container should do when it is started - in this case, run a binary file called "main" which was compiled by the go compiler during our container build process.

Once the container image is built, OpenShift will store the image in its internal container image repository and then proceed to automatically create the necessary Kubernetes objects to run our container imagine - including the deployment, service and OpenShift route objects. OpenShift builds really simplify the process of getting application code into production quickly for developers.


[TIP]
.A note on Dockerfiles
====
You may encounter the term "Dockerfile" which was traditionally used to define how a container image should be built. However, Dockerfiles are now largely superseded by Containerfiles which are backwards compatible with the traditional Dockerfile. You will learn more about the container build process in section 10 of this course.
====

Specify the container image file as follows:

image::08-06-containerfile.png[container root filesystem,700,align="center"]


Next, we click the blue "Create" button at the bottom of the page that will start the build process.

You will see the application build is in progress:

image::08-06a-building.png[app building,700,align="center"]


[#test_storage_persistance]
== Test our Storage persistance

Now that we have a container running inside of OpenShift we can investigate how OpenShift handles storage persistence following on from what we learnt earlier.

=== Step 1 - Without a Persistent Volume

Firstly, lets investigate persistence without a Persistent Volume attached to our container. We'll start by accessing the terminal for our running container. We firstly need to ensure that we have switched from "Developer View", to "Administrator View" (Top left drop down menu). Next, choose the menu option "Pods" from the "Workloads" drop down on the left hand side of the screen. You should see two pods, one in a "Running" status and the other in a "Completed" status. Click the name of the pod which has the status "Running".

We now want to access the terminal, so choose that tab.

For this test we are simply going to create a text file on the filesystem, scale the pod down and then back up and see if our change has persisted.

Once in the terminal type the following:

[.console-input]
[source,bash,subs="+attributes"]
----
echo "Testing Persistence" > /data/test.txt
----

The above command simple creates a text file called "test.txt" in the folder "data", that contains the message "Testing Persistence".

Once that file is created, we'll verify everything went to plan:

[.console-input]
[source,bash,subs="+attributes"]
----
cat /data/test.txt
----

You should have the message "Testing Persistence" displayed in the terminal window. (We have used cat in this exmaple, to read the contents of test.txt and display to our terminal window)

[TIP]
.A note on stdin and stdout
====
We have just used two linux commands, echo and cat which make use of input and output streams within Linux known as stdin and stdout. You can read more here:
http://www.howtogeek.com/435903/what-are-stdin-stdout-and-stderr-on-linux/[stdin, stdout,stderr]
====


image::08-07-verify-test.png[container root filesystem,700,align="center"]

This should not be a suprise at this point, as the container is currently running within the pod. Remember, this filesystem is volatile - it is in memory and not actually persisted to any storage device. We can demonstrate this by "restarting the container". - We don't actually restart containers, we destroy and rebuild them. To do this, we will "scale down" our "deployment". 

Whilst in the "Administrator" view, choose the drop down menu "Workloads" and then selected "Deployments". You should see the deployment that represents our running application listed as "bootcamp-webserver-git". Select the name (which is a link) and you should see beneath "Deployment details" a pair of "up and down" controls. This simply controls how many pods are deployed within the deployment. Scale this down to zero, which will destroy our current running container.

image::08-08-scale-down-pod.png[container root filesystem,300,align="center"]

And next, scale the deployment back up to one, which will recreate our container.

image::08-09-scale-up-pod.png[container root filesystem,300,align="center"]

With the container running again we will re-run our simple test of taking the content of the file "test.txt" and display it on the terminal window. Head over to the terminal window for the newly created container. (Workloads -> Pods -> (select pod) -> Terminal)

And simply enter:

[.console-input]
[source,bash,subs="+attributes"]
----
cat /data/test.txt
----

You should see:

image::08-10-test-persistence.png[container root filesystem,700,align="center"]

Which confirms what we have discussed so far. By default, containers are ephemeral and do not have persistent storage. To make useful applications they need to either connect to an external data source (such as a database) or, have a volume mounted to them for persistence.

=== Step 2 - With a Persistent Volume

Now we will solve the problem of data persistence by using OpenShift to provide a persistent volume to our container. 

Choose the "Workloads" drop down once more and then select "Deployments". Towards the far right of the window you should be able to see an options menu in the form of a kebab menu.:

image::08-11b-deployment-storage.png[container root filesystem,700,align="center"]

Choose "Add Storage".

Next, we need to configure what type of storage we want to use. We could, use an existing storage claim if we had one but we want to create a new claim. Choose "Create new Claim".

Once we select "Create new claim", we will see a number of options expand. The first option defines our StorageClass. In the screen shot this is of type "gp2" which represents a type of cloud storage in Amazon Web Services (AWS) that is specific to AWS. StorageClasses allow us to abstract a layer of complexity away for us by not having to worry about the differences between the different combinations of storage and storage vendor. - This could be Auzre, Google Cloud, NetAPP storage etc - as long as the storage adheres to a set of standards specifically designed for Kubernetes, OpenShift will be able to use it.

We need to name our PersistentVolumeClaim, enter:

[.console-input]
[source]
----
bootcamp-webserver-pvc
----

Choose a size of 1 GiB.

Next, we will specify the mount path. As mentioned earlier, we map a volume to a path inside our container's filesystem. It is the /data directory that we want to make persistent, so enter:

[.console-input]
[source]
----
/data
----

We should have the following at this stage:



image::08-11c-add-storage-details.png[storage details,800,align="center"]

Click save. Once the persistent volume claim has been submitted, the OpenShift user interface will switch back to the deployment depicting that a scaling event has started:

image::08-12-pvc-post-scale.png[container root filesystem,700,align="center"]

Once the pod has scaled, we can once again try our test from earlier. Switch to the terminal for the container, and create a text file within the /data folder which we have defined as a persistently mounted volume. As before amend the deployment to scale the pod down to zero, and then back to one. Investigate and see that the file this time, has persisted.


image::08-13-testing-pvc.png[container root filesystem,700,align="center"]

[#add_storage_to_workshop]
== Adding a persistent volume to our Bootcamp app

For the next stages of this course, we need to add some persistent storage to our bootcampapp deploy. Switch to the project that you deployed the Open JDK Bootcamp application to previously in section three. As before, in Administrator view, choose "Workloads" and then "Deployments".

You should see a deployment similar to the following:

image::08-14-bootcamp-deployment.png[deployment filesystem,700,align="center"]

As before we are going to create a PersistentVolumeClaim using a StorageClass. From the kebab icon menu, choose "Add Storage". We will use the same options as before, however with the PersistentVolumeClaim name:

[.console-input]
[source]
----
bootcampapp-pvc
----

As follows:

image::08-15-bootcamp-storage-details.png[storage details filesystem,700,align="center"]

After OpenShift scales the pod down and then back up again, the bootcamp application from this point forward will have persistent storage if the /data directory is used within the container's internal filesystem


[NOTE]
====
If you would like to learn more in depth about storage within Kubernetes / OpenShift, please read the following documentation provided by the Kubernetes community:
https://kubernetes.io/docs/concepts/storage/persistent-volumes/
====